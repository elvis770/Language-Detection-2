{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENT THE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ich denke es handelt sich hier um ein missvers...</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ich habe tom gerade erst verlassen</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tom versuchte mary nur zu 채rgern</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tom hat mir die hand gek체sst</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ich wusste dass dir das gefiele</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text language\n",
       "0  ich denke es handelt sich hier um ein missvers...   german\n",
       "1                 ich habe tom gerade erst verlassen   german\n",
       "2                   tom versuchte mary nur zu 채rgern   german\n",
       "3                       tom hat mir die hand gek체sst   german\n",
       "4                    ich wusste dass dir das gefiele   german"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/elvisechefu/Desktop/language detection/languages.csv')\n",
    "\n",
    "labels = df['language'].values\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS THE TEXT : LOWERCASING, TOKENIZATION, REMOVAL OF PUNCTUATION< SPECAIL CHARACTERS AND STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "df['clean_text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "df['tokenized_text'] = df['clean_text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Removing Punctuation and Special Characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(remove_special_characters)\n",
    "\n",
    "# Removing Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "df['preprocessed_text'] = df['tokenized_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract character-level features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_char_features(document, word_length=4):\n",
    "    document = re.sub(r'[^a-zA-Z0-9\\s]', '', document)\n",
    "    features = {}\n",
    "    for i in range(1, word_length+1):\n",
    "        features['char-%s' % i] = [0]*26\n",
    "        for j in range(len(document)-i+1):\n",
    "            try:\n",
    "                features['char-%s' % i][ord(document[j]) - ord('a')] += 1\n",
    "            except:\n",
    "                features['char-%s' % i][ord(document[j]) - ord('A')] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT CHARACTER-LEVEL FEATURES FOR EACH DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['preprocessed_text']:  # Check if the list is not empty\n",
    "        X.append(extract_char_features(row['preprocessed_text'][0]))\n",
    "    else:\n",
    "        # Handle the case where the list is empty\n",
    "        # For example, you could append a placeholder value or skip this row\n",
    "        pass\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT THE DATASET INTO TEST VALIDATE AND TRAIN SET\n",
    "Whilst trying to split the data i found some duplicates in the labels, so i removed the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extra labels\n",
    "aligned_labels = labels[:len(X)]\n",
    "\n",
    "labels = df[df['preprocessed_text'].apply(bool)]['language'].values\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dictionaries to 2D numpy array\n",
    "X_train_array = np.array([np.array(list(d.values())) for d in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM_CLF is SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM classifier\n",
    "svm_clf = SVC(kernel='linear', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming X_train is a list of dictionaries\n",
    "# Flatten values from each dictionary and stack them into an array\n",
    "X_train_array = np.array([np.array(list(d.values())).flatten() for d in X_train])\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "svm_clf.fit(X_train_array, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
